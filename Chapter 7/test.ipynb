{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3328d63d-4ec3-4c22-b4b0-7cb48450c0ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reinforcement Learning - Sutton and Barto\n",
    "# Exercise 7.2\n",
    "\n",
    "n-step TD Methods Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb34b0a7-000d-4ec4-ac03-6559d69b9462",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from RandomWalk import randomwalk\n",
    "import n_step_TD as agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336514a4-6658-4a20-9616-9f28d0bd85f2",
   "metadata": {},
   "source": [
    "# Random Walk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a7e774-aa3c-4ad8-b58d-e7471df0911d",
   "metadata": {},
   "source": [
    "## Initialize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a735ab69-91e7-4069-bdd4-5792926319a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions: [array([0, 1]), array([ 0, -1])]\n"
     ]
    }
   ],
   "source": [
    "env = randomwalk()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccd79d7-3da3-4fb9-90ce-e9d91b86d0eb",
   "metadata": {},
   "source": [
    "# Run with defined state values and determined sequence of actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65023c94-dd3f-4d25-b7a9-a8fb806643dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Values: [[0.  0.5 0.5 0.5 0.5 0.5 0. ]]\n"
     ]
    }
   ],
   "source": [
    "size = env.observation_space[1]\n",
    "\n",
    "n = 4\n",
    "state_values = agent.initialize_state_values((env.observation_space[0],env.observation_space[1]))\n",
    "print(f\"State Values: {state_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47d061cb-236f-4d4d-b053-a038aa09ff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [0,1,1,0,0,1,1,0,0,1,0,1,0,1,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4454dc92-6fb0-4d92-90c0-797de87b481c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "Episode 1\n",
      "Action: 0\n",
      "Step: 0 | State: [0 3] | Action: 0 | Next State: [0 4] | Reward: 0 | State Values: [0.5 0.5 0.5 0.5 0.5] | Step: 0 | T: inf | time: -3\n",
      "Action: 1\n",
      "Step: 1 | State: [0 4] | Action: 1 | Next State: [0 3] | Reward: 0 | State Values: [0.5 0.5 0.5 0.5 0.5] | Step: 1 | T: inf | time: -2\n",
      "Action: 1\n",
      "Step: 2 | State: [0 3] | Action: 1 | Next State: [0 2] | Reward: 0 | State Values: [0.5 0.5 0.5 0.5 0.5] | Step: 2 | T: inf | time: -1\n",
      "Action: 0\n",
      "Step: 3 | State: [0 2] | Action: 0 | Next State: [0 3] | Reward: 0 | State Values: [0.5 0.5 0.5 0.5 0.5] | Step: 3 | T: inf | time: 0\n",
      "Action: 0\n",
      "Step: 4 | State: [0 3] | Action: 0 | Next State: [0 4] | Reward: 0 | State Values: [0.5 0.5 0.5 0.5 0.5] | Step: 4 | T: inf | time: 1\n",
      "Action: 1\n",
      "Step: 5 | State: [0 4] | Action: 1 | Next State: [0 3] | Reward: 0 | State Values: [0.5 0.5 0.5 0.5 0.5] | Step: 5 | T: inf | time: 2\n",
      "Action: 1\n",
      "Step: 6 | State: [0 3] | Action: 1 | Next State: [0 2] | Reward: 0 | State Values: [0.5 0.5 0.5 0.5 0.5] | Step: 6 | T: inf | time: 3\n",
      "Action: 0\n",
      "Step: 7 | State: [0 2] | Action: 0 | Next State: [0 3] | Reward: 0 | State Values: [0.5 0.5 0.5 0.5 0.5] | Step: 7 | T: inf | time: 4\n",
      "Action: 0\n",
      "Step: 8 | State: [0 3] | Action: 0 | Next State: [0 4] | Reward: 0 | State Values: [0.5 0.5 0.5 0.5 0.5] | Step: 8 | T: inf | time: 5\n",
      "Action: 1\n",
      "Step: 9 | State: [0 4] | Action: 1 | Next State: [0 3] | Reward: 0 | State Values: [0.5 0.5 0.5 0.5 0.5] | Step: 9 | T: inf | time: 6\n",
      "Action: 0\n",
      "Step: 10 | State: [0 3] | Action: 0 | Next State: [0 4] | Reward: 0 | State Values: [0.5 0.5 0.5 0.5 0.5] | Step: 10 | T: inf | time: 7\n",
      "Action: 1\n",
      "Step: 11 | State: [0 4] | Action: 1 | Next State: [0 3] | Reward: 0 | State Values: [0.5 0.5 0.5 0.5 0.5] | Step: 11 | T: inf | time: 8\n",
      "Action: 0\n",
      "Step: 12 | State: [0 3] | Action: 0 | Next State: [0 4] | Reward: 0 | State Values: [0.5 0.5 0.5 0.5 0.5] | Step: 12 | T: inf | time: 9\n",
      "Action: 1\n",
      "Step: 13 | State: [0 4] | Action: 1 | Next State: [0 3] | Reward: 0 | State Values: [0.5 0.5 0.5 0.5 0.5] | Step: 13 | T: inf | time: 10\n",
      "Action: 0\n",
      "Step: 14 | State: [0 3] | Action: 0 | Next State: [0 4] | Reward: 0 | State Values: [0.5 0.5 0.5 0.5 0.5] | Step: 14 | T: inf | time: 11\n",
      "Action: 0\n",
      "Step: 15 | State: [0 4] | Action: 0 | Next State: [0 5] | Reward: 0 | State Values: [0.5 0.5 0.5 0.5 0.5] | Step: 15 | T: inf | time: 12\n",
      "Action: 0\n",
      "Step: 16 | State: [0 5] | Action: 0 | Next State: [0 6] | Reward: 1 | State Values: [0.5  0.5  0.5  0.55 0.5 ] | Step: 16 | T: 17 | time: 13\n",
      "Step: 17 | State: [0 6] | Action: 0 | Next State: [0 6] | Reward: 1 | State Values: [0.5  0.5  0.55 0.55 0.5 ] | Step: 17 | T: 17 | time: 14\n",
      "Step: 18 | State: [0 6] | Action: 0 | Next State: [0 6] | Reward: 1 | State Values: [0.5   0.5   0.55  0.595 0.5  ] | Step: 18 | T: 17 | time: 15\n",
      "Step: 19 | State: [0 6] | Action: 0 | Next State: [0 6] | Reward: 1 | State Values: [0.5   0.5   0.55  0.595 0.55 ] | Step: 19 | T: 17 | time: 16\n",
      "alpha: 0.1\n",
      "Total number of episodes: 1\n",
      "Total number of steps: 0\n",
      "Total Average of Steps Per Episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "n_step_td_history = agent.n_step_td_estimating(env, state_values, episodes=1, n=n, alpha=0.1, actions=actions, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d7cccdd-b0cd-4777-9ba7-54dae55eed07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.5  , 0.5  , 0.55 , 0.595, 0.55 ])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_step_td_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8e5c568-84db-4c97-aafc-2cb6de66ef1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Values: [[0.    0.5   0.5   0.55  0.595 0.55  0.   ]]\n"
     ]
    }
   ],
   "source": [
    "state_values[0][0] = 0.\n",
    "state_values[0][6] = 0.\n",
    "for i in range(5):\n",
    "    state_values[0][i+1] = n_step_td_history[0][i]\n",
    "print(f\"State Values: {state_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d652f710-29f3-4323-9411-b7e3458b0572",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [0,1,0,0,1,1,1,0,1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "610f698b-15a7-4ac9-a581-22bf01811907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "Episode 1\n",
      "Action: 0\n",
      "Step: 0 | State: [0 3] | Action: 0 | Next State: [0 4] | Reward: 0 | State Values: [0.5   0.5   0.55  0.595 0.55 ] | Step: 0 | T: inf | time: -3\n",
      "Action: 1\n",
      "Step: 1 | State: [0 4] | Action: 1 | Next State: [0 3] | Reward: 0 | State Values: [0.5   0.5   0.55  0.595 0.55 ] | Step: 1 | T: inf | time: -2\n",
      "Action: 0\n",
      "Step: 2 | State: [0 3] | Action: 0 | Next State: [0 4] | Reward: 0 | State Values: [0.5   0.5   0.55  0.595 0.55 ] | Step: 2 | T: inf | time: -1\n",
      "Action: 0\n",
      "Step: 3 | State: [0 4] | Action: 0 | Next State: [0 5] | Reward: 0 | State Values: [0.5   0.5   0.55  0.595 0.55 ] | Step: 3 | T: inf | time: 0\n",
      "Action: 1\n",
      "Step: 4 | State: [0 5] | Action: 1 | Next State: [0 4] | Reward: 0 | State Values: [0.5   0.5   0.55  0.595 0.55 ] | Step: 4 | T: inf | time: 1\n",
      "Action: 1\n",
      "Step: 5 | State: [0 4] | Action: 1 | Next State: [0 3] | Reward: 0 | State Values: [0.5   0.5   0.55  0.595 0.55 ] | Step: 5 | T: inf | time: 2\n",
      "Action: 1\n",
      "Step: 6 | State: [0 3] | Action: 1 | Next State: [0 2] | Reward: 0 | State Values: [0.5    0.5    0.55   0.5855 0.55  ] | Step: 6 | T: inf | time: 3\n",
      "Action: 0\n",
      "Step: 7 | State: [0 2] | Action: 0 | Next State: [0 3] | Reward: 0 | State Values: [0.5    0.5    0.55   0.5855 0.55  ] | Step: 7 | T: inf | time: 4\n",
      "Action: 1\n",
      "Step: 8 | State: [0 3] | Action: 1 | Next State: [0 2] | Reward: 0 | State Values: [0.5     0.5     0.55    0.57695 0.55   ] | Step: 8 | T: inf | time: 5\n",
      "Action: 1\n",
      "Step: 9 | State: [0 2] | Action: 1 | Next State: [0 1] | Reward: 0 | State Values: [0.5     0.5     0.545   0.57695 0.55   ] | Step: 9 | T: inf | time: 6\n",
      "Action: 1\n",
      "Step: 10 | State: [0 1] | Action: 1 | Next State: [0 0] | Reward: 0 | State Values: [0.5     0.45    0.545   0.57695 0.55   ] | Step: 10 | T: 11 | time: 7\n",
      "Step: 11 | State: [0 0] | Action: 1 | Next State: [0 0] | Reward: 0 | State Values: [0.5     0.45    0.4905  0.57695 0.55   ] | Step: 11 | T: 11 | time: 8\n",
      "Step: 12 | State: [0 0] | Action: 1 | Next State: [0 0] | Reward: 0 | State Values: [0.5     0.405   0.4905  0.57695 0.55   ] | Step: 12 | T: 11 | time: 9\n",
      "Step: 13 | State: [0 0] | Action: 1 | Next State: [0 0] | Reward: 0 | State Values: [0.45    0.405   0.4905  0.57695 0.55   ] | Step: 13 | T: 11 | time: 10\n",
      "alpha: 0.1\n",
      "Total number of episodes: 1\n",
      "Total number of steps: 0\n",
      "Total Average of Steps Per Episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "n_step_td_history = agent.n_step_td_estimating(env, state_values, episodes=1, n=n, alpha=0.1, actions=actions, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5151a5d4-bef0-4e9e-86df-6abbe6d7b432",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.45   , 0.405  , 0.4905 , 0.57695, 0.55   ])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_step_td_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46d1afd0-d671-4529-8b05-e14995f7bf35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Values: [[0.      0.45    0.405   0.4905  0.57695 0.55    0.     ]]\n"
     ]
    }
   ],
   "source": [
    "state_values[0][0] = 0.\n",
    "state_values[0][6] = 0.\n",
    "for i in range(5):\n",
    "    state_values[0][i+1] = n_step_td_history[0][i]\n",
    "print(f\"State Values: {state_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1fbf15b-74f2-4d71-a265-66e0888e51ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [0,1,1,0,1,1,0,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77ce1525-315c-4b2b-a4fe-3392045e5795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "Episode 1\n",
      "Action: 0\n",
      "Step: 0 | State: [0 3] | Action: 0 | Next State: [0 4] | Reward: 0 | State Values: [0.45    0.405   0.4905  0.57695 0.55   ] | Step: 0 | T: inf | time: -3\n",
      "Action: 1\n",
      "Step: 1 | State: [0 4] | Action: 1 | Next State: [0 3] | Reward: 0 | State Values: [0.45    0.405   0.4905  0.57695 0.55   ] | Step: 1 | T: inf | time: -2\n",
      "Action: 1\n",
      "Step: 2 | State: [0 3] | Action: 1 | Next State: [0 2] | Reward: 0 | State Values: [0.45    0.405   0.4905  0.57695 0.55   ] | Step: 2 | T: inf | time: -1\n",
      "Action: 0\n",
      "Step: 3 | State: [0 2] | Action: 0 | Next State: [0 3] | Reward: 0 | State Values: [0.45    0.405   0.4905  0.57695 0.55   ] | Step: 3 | T: inf | time: 0\n",
      "Action: 1\n",
      "Step: 4 | State: [0 3] | Action: 1 | Next State: [0 2] | Reward: 0 | State Values: [0.45     0.405    0.4905   0.559755 0.55    ] | Step: 4 | T: inf | time: 1\n",
      "Action: 1\n",
      "Step: 5 | State: [0 2] | Action: 1 | Next State: [0 1] | Reward: 0 | State Values: [0.45     0.405    0.48645  0.559755 0.55    ] | Step: 5 | T: inf | time: 2\n",
      "Action: 0\n",
      "Step: 6 | State: [0 1] | Action: 0 | Next State: [0 2] | Reward: 0 | State Values: [0.45     0.405    0.48645  0.559755 0.55    ] | Step: 6 | T: inf | time: 3\n",
      "Action: 1\n",
      "Step: 7 | State: [0 2] | Action: 1 | Next State: [0 1] | Reward: 0 | State Values: [0.45     0.405    0.482805 0.559755 0.55    ] | Step: 7 | T: inf | time: 4\n",
      "Action: 1\n",
      "Step: 8 | State: [0 1] | Action: 1 | Next State: [0 0] | Reward: 0 | State Values: [0.45     0.3645   0.482805 0.559755 0.55    ] | Step: 8 | T: 9 | time: 5\n",
      "Step: 9 | State: [0 0] | Action: 1 | Next State: [0 0] | Reward: 0 | State Values: [0.405    0.3645   0.482805 0.559755 0.55    ] | Step: 9 | T: 9 | time: 6\n",
      "Step: 10 | State: [0 0] | Action: 1 | Next State: [0 0] | Reward: 0 | State Values: [0.405    0.32805  0.482805 0.559755 0.55    ] | Step: 10 | T: 9 | time: 7\n",
      "Step: 11 | State: [0 0] | Action: 1 | Next State: [0 0] | Reward: 0 | State Values: [0.3645   0.32805  0.482805 0.559755 0.55    ] | Step: 11 | T: 9 | time: 8\n",
      "alpha: 0.1\n",
      "Total number of episodes: 1\n",
      "Total number of steps: 0\n",
      "Total Average of Steps Per Episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "n_step_td_history = agent.n_step_td_estimating(env, state_values, episodes=1, n=n, alpha=0.1, actions=actions, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af78b6fd-04bb-45be-8ced-4eb87fdc1e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.3645  , 0.32805 , 0.482805, 0.559755, 0.55    ])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_step_td_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97d93afe-8bdb-481b-b958-035a88ac6b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Values: [[0.       0.3645   0.32805  0.482805 0.559755 0.55     0.      ]]\n"
     ]
    }
   ],
   "source": [
    "state_values[0][0] = 0.\n",
    "state_values[0][6] = 0.\n",
    "for i in range(5):\n",
    "    state_values[0][i+1] = n_step_td_history[0][i]\n",
    "print(f\"State Values: {state_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4105e2ee-c6bb-40b6-be53-21e47e4e18cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [1,0,0,1,1,1,0,1,0,0,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f02bb018-8397-4d69-8a11-407038a8e5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "Episode 1\n",
      "Action: 1\n",
      "Step: 0 | State: [0 3] | Action: 1 | Next State: [0 2] | Reward: 0 | State Values: [0.3645   0.32805  0.482805 0.559755 0.55    ] | Step: 0 | T: inf | time: -3\n",
      "Action: 0\n",
      "Step: 1 | State: [0 2] | Action: 0 | Next State: [0 3] | Reward: 0 | State Values: [0.3645   0.32805  0.482805 0.559755 0.55    ] | Step: 1 | T: inf | time: -2\n",
      "Action: 0\n",
      "Step: 2 | State: [0 3] | Action: 0 | Next State: [0 4] | Reward: 0 | State Values: [0.3645   0.32805  0.482805 0.559755 0.55    ] | Step: 2 | T: inf | time: -1\n",
      "Action: 1\n",
      "Step: 3 | State: [0 4] | Action: 1 | Next State: [0 3] | Reward: 0 | State Values: [0.3645   0.32805  0.482805 0.559755 0.55    ] | Step: 3 | T: inf | time: 0\n",
      "Action: 1\n",
      "Step: 4 | State: [0 3] | Action: 1 | Next State: [0 2] | Reward: 0 | State Values: [0.3645   0.32805  0.482805 0.559755 0.55    ] | Step: 4 | T: inf | time: 1\n",
      "Action: 1\n",
      "Step: 5 | State: [0 2] | Action: 1 | Next State: [0 1] | Reward: 0 | State Values: [0.3645    0.32805   0.4709745 0.559755  0.55     ] | Step: 5 | T: inf | time: 2\n",
      "Action: 0\n",
      "Step: 6 | State: [0 1] | Action: 0 | Next State: [0 2] | Reward: 0 | State Values: [0.3645    0.32805   0.4709745 0.5365845 0.55     ] | Step: 6 | T: inf | time: 3\n",
      "Action: 1\n",
      "Step: 7 | State: [0 2] | Action: 1 | Next State: [0 1] | Reward: 0 | State Values: [0.3645     0.32805    0.46032705 0.5365845  0.55      ] | Step: 7 | T: inf | time: 4\n",
      "Action: 0\n",
      "Step: 8 | State: [0 1] | Action: 0 | Next State: [0 2] | Reward: 0 | State Values: [0.3645     0.32805    0.46032705 0.5365845  0.55      ] | Step: 8 | T: inf | time: 5\n",
      "Action: 0\n",
      "Step: 9 | State: [0 2] | Action: 0 | Next State: [0 3] | Reward: 0 | State Values: [0.37408271 0.32805    0.46032705 0.5365845  0.55      ] | Step: 9 | T: inf | time: 6\n",
      "Action: 0\n",
      "Step: 10 | State: [0 3] | Action: 0 | Next State: [0 4] | Reward: 0 | State Values: [0.37408271 0.34890345 0.46032705 0.5365845  0.55      ] | Step: 10 | T: inf | time: 7\n",
      "Action: 0\n",
      "Step: 11 | State: [0 4] | Action: 0 | Next State: [0 5] | Reward: 0 | State Values: [0.39167443 0.34890345 0.46032705 0.5365845  0.55      ] | Step: 11 | T: inf | time: 8\n",
      "Action: 0\n",
      "Step: 12 | State: [0 5] | Action: 0 | Next State: [0 6] | Reward: 1 | State Values: [0.39167443 0.41401311 0.46032705 0.5365845  0.55      ] | Step: 12 | T: 13 | time: 9\n",
      "Step: 13 | State: [0 6] | Action: 0 | Next State: [0 6] | Reward: 1 | State Values: [0.39167443 0.41401311 0.51429435 0.5365845  0.55      ] | Step: 13 | T: 13 | time: 10\n",
      "Step: 14 | State: [0 6] | Action: 0 | Next State: [0 6] | Reward: 1 | State Values: [0.39167443 0.41401311 0.51429435 0.58292605 0.55      ] | Step: 14 | T: 13 | time: 11\n",
      "Step: 15 | State: [0 6] | Action: 0 | Next State: [0 6] | Reward: 1 | State Values: [0.39167443 0.41401311 0.51429435 0.58292605 0.595     ] | Step: 15 | T: 13 | time: 12\n",
      "alpha: 0.1\n",
      "Total number of episodes: 1\n",
      "Total number of steps: 0\n",
      "Total Average of Steps Per Episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "n_step_td_history = agent.n_step_td_estimating(env, state_values, episodes=1, n=n, alpha=0.1, actions=actions, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "356bf7a2-0552-4304-b76e-487496641858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.39167443, 0.41401311, 0.51429435, 0.58292605, 0.595     ])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_step_td_history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc-showcode": true,
  "toc-showmarkdowntxt": true,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
