{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Exercise 8.4 - Dyna-Q+ with Action Exploration Bonus\n",
    "\n",
    "The exploration bonus described above actually changes the estimated values of states and\n",
    "actions. Is this necessary? Suppose the bonus $\\kappa \\sqrt{\\tau}$ was used not in\n",
    "updates, but solely in action selection. That is, suppose the action selected was always\n",
    "that for which $Q(S_t,a) + \\kappa \\sqrt{\\tau(S_t,a)}$ was maximal. Carry out a gridworld\n",
    "experiment that tests and illustrates the strengths and weaknesses of this alternate\n",
    "approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! pip3 -q install numpy matplotlib gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import maze\n",
    "from dyna import dyna_q, dyna_q_plus"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Reproduction of Figure 8.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('ShortcutMaze-v0')\n",
    "env.seed(3); np.random.seed(3)\n",
    "q0, policy0, history0 = dyna_q(env, n=50, num_episodes=400, alpha=0.5, gamma=0.95)\n",
    "\n",
    "env = gym.make('ShortcutMaze-v0')\n",
    "env.seed(3); np.random.seed(3)\n",
    "q1, policy1, history1 = dyna_q_plus(env, n=50, num_episodes=400, alpha=0.5, gamma=0.95,\n",
    "                                    kappa=5e-4, action_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_learning(histories, labels):\n",
    "    with matplotlib.rc_context({'figure.figsize': [10, 10]}):\n",
    "        plt.plot([0, 1])\n",
    "        plt.figure()\n",
    "        plt.title(f\"Cumulative Reward\")\n",
    "        plt.xlabel(\"Timesteps\")\n",
    "        plt.ylabel(\"Cumulative Reward\")\n",
    "    \n",
    "        plt.xlim([0, 6_000])\n",
    "        plt.ylim([0, 400])\n",
    "        \n",
    "        for history, label in zip(histories, labels):\n",
    "            plt.plot(np.cumsum(history), label=label)\n",
    "    \n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "def plot_results(env, q, policy, label):\n",
    "    # Render example episode\n",
    "    done, state = False, env.reset()\n",
    "    env.render()\n",
    "    while not done:\n",
    "        action = np.random.choice(env.action_space.n, p=policy[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "        env.render()\n",
    "    plt.title(f\"Example Episode ({label})\")\n",
    "\n",
    "    env = env.unwrapped\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca()\n",
    "    ax.set_title(f\"Value Function and Policy ({label})\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    # Plot value function\n",
    "    q = np.copy(q)\n",
    "    unvisited = np.where(q == 0)\n",
    "    q[unvisited] = -np.inf\n",
    "    v = np.max(q, axis=1).reshape(env.observation_space.nvec)\n",
    "    ax.imshow(v.T, origin='lower')\n",
    "\n",
    "    # Plot actions of the policy\n",
    "    a_stars = np.argmax(policy, axis=1)\n",
    "    arrows = np.array([env.actions[a] for a in a_stars])\n",
    "    arrows[unvisited[0], :] = 0\n",
    "    arrows = arrows.reshape([*env.observation_space.nvec, 2])\n",
    "    xr = np.arange(env.observation_space.nvec[0])\n",
    "    yr = np.arange(env.observation_space.nvec[1])\n",
    "    ax.quiver(xr, yr, arrows[:, :, 0].T, arrows[:, :, 1].T, pivot='mid', scale=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_stars = np.argmax(policy0, axis=1)\n",
    "arrows = np.array([env.actions[a] for a in a_stars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_results(env, q0, policy0, \"Dyna-Q\")\n",
    "plot_results(env, q1, policy1, \"Dyna-Q+\")\n",
    "\n",
    "plot_learning([history0, history1], [\"Dyna-Q\", \"Dyna-Q+\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Benchmark of Dyna-Q+ with Action Only Exploration Bonus\n",
    "\n",
    "Using exploration bonus for actions only resulted in faster learning in the initial phase before\n",
    "the shortcut was opened. Howerver the exploration bonus wasn't stable enough to discover and\n",
    "exploit the shortcut. The bonus affects only a single action selection and isn't able to\n",
    "systematically map out areas unvisited for a long time.\n",
    " \n",
    "Dyna-Q+ directly increases action values of unvisited areas it's able to create a map of the\n",
    "long since visited territory and plan periodic repeated visits to these states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('ShortcutMaze-v0')\n",
    "env.seed(3); np.random.seed(3)\n",
    "q2, policy2, history2 = dyna_q_plus(env, n=50, num_episodes=400, alpha=0.5, gamma=0.95,\n",
    "                                    kappa=5e-4, action_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_results(env, q2, policy2, \"Dyna-Q+ with Action Exploration Bonus\")\n",
    "\n",
    "plot_learning([history0, history1, history2],\n",
    "              [\"Dyna-Q\", \"Dyna-Q+\", \"Dyna-Q+ with Action Exploration Bonus\"])"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "nteract": {
   "version": "0.14.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
