{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a0ce497-0ade-47e5-ac09-a8ef3e9beff9",
   "metadata": {},
   "source": [
    "## Exercise 8.4 - Dyna-Q+ with Action Exploration Bonus\n",
    "\n",
    "The exploration bonus described above actually changes the estimated values of states and\n",
    "actions. Is this necessary? Suppose the bonus $\\kappa \\sqrt{\\tau}$ was used not in\n",
    "updates, but solely in action selection. That is, suppose the action selected was always\n",
    "that for which $Q(S_t,a) + \\kappa \\sqrt{\\tau(S_t,a)}$ was maximal. Carry out a gridworld\n",
    "experiment that tests and illustrates the strengths and weaknesses of this alternate\n",
    "approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "779f32e4-4278-41bd-8378-10de002110ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from maze_c import maze_c\n",
    "from dyna_c import dyna_q, dyna_q_plus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87b7133-c767-4d4a-a0d0-8dc920c6aecd",
   "metadata": {},
   "source": [
    "### Reproduction of Figure 8.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42f12da6-da0c-4fed-9abf-b5f225b6683f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'n'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m env \u001b[38;5;241m=\u001b[39m maze_c()\n\u001b[0;32m      2\u001b[0m env\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m3\u001b[39m); np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m q0, policy0, history0 \u001b[38;5;241m=\u001b[39m \u001b[43mdyna_q\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m env \u001b[38;5;241m=\u001b[39m maze_c()\n\u001b[0;32m      6\u001b[0m env\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m3\u001b[39m); np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[1;32m~\\projects\\RL_Book\\Chapter 8\\dyna_c.py:9\u001b[0m, in \u001b[0;36mdyna_q\u001b[1;34m(env, n, num_episodes, eps, alpha, gamma)\u001b[0m\n\u001b[0;32m      6\u001b[0m history \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Number of available actions and maximal state ravel index\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m n_state, n_action \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn\u001b[49m, env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Initialization of action value function\u001b[39;00m\n\u001b[0;32m     12\u001b[0m q \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros([n_state, n_action], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'n'"
     ]
    }
   ],
   "source": [
    "env = maze_c()\n",
    "env.seed(3); np.random.seed(3)\n",
    "q0, policy0, history0 = dyna_q(env, n=50, num_episodes=400, alpha=0.5, gamma=0.95)\n",
    "\n",
    "env = maze_c()\n",
    "env.seed(3); np.random.seed(3)\n",
    "q1, policy1, history1 = dyna_q_plus(env, n=50, num_episodes=400, alpha=0.5, gamma=0.95,\n",
    "                                    kappa=5e-4, action_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36daddad-3d9f-456e-99e4-64cccc9cc513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning(histories, labels):\n",
    "    with matplotlib.rc_context({'figure.figsize': [10, 10]}):\n",
    "        plt.plot([0, 1])\n",
    "        plt.figure()\n",
    "        plt.title(f\"Cumulative Reward\")\n",
    "        plt.xlabel(\"Timesteps\")\n",
    "        plt.ylabel(\"Cumulative Reward\")\n",
    "    \n",
    "        plt.xlim([0, 6_000])\n",
    "        plt.ylim([0, 400])\n",
    "        \n",
    "        for history, label in zip(histories, labels):\n",
    "            plt.plot(np.cumsum(history), label=label)\n",
    "    \n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "def plot_results(env, q, policy, label):\n",
    "    # Render example episode\n",
    "    done, state = False, env.reset()\n",
    "    env.render()\n",
    "    while not done:\n",
    "        action = np.random.choice(env.action_space.n, p=policy[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "        env.render()\n",
    "    plt.title(f\"Example Episode ({label})\")\n",
    "\n",
    "    env = env.unwrapped\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca()\n",
    "    ax.set_title(f\"Value Function and Policy ({label})\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    # Plot value function\n",
    "    q = np.copy(q)\n",
    "    unvisited = np.where(q == 0)\n",
    "    q[unvisited] = -np.inf\n",
    "    v = np.max(q, axis=1).reshape(env.observation_space.nvec)\n",
    "    ax.imshow(v.T, origin='lower')\n",
    "\n",
    "    # Plot actions of the policy\n",
    "    a_stars = np.argmax(policy, axis=1)\n",
    "    arrows = np.array([env.actions[a] for a in a_stars])\n",
    "    arrows[unvisited[0], :] = 0\n",
    "    arrows = arrows.reshape([*env.observation_space.nvec, 2])\n",
    "    xr = np.arange(env.observation_space.nvec[0])\n",
    "    yr = np.arange(env.observation_space.nvec[1])\n",
    "    ax.quiver(xr, yr, arrows[:, :, 0].T, arrows[:, :, 1].T, pivot='mid', scale=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c43854-04a0-4e0c-8566-8602d94320d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(env, q0, policy0, \"Dyna-Q\")\n",
    "plot_results(env, q1, policy1, \"Dyna-Q+\")\n",
    "\n",
    "plot_learning([history0, history1], [\"Dyna-Q\", \"Dyna-Q+\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f085c68-fa3b-4058-95ab-2df3492d8fd8",
   "metadata": {},
   "source": [
    "### Benchmark of Dyna-Q+ with Action Only Exploration Bonus\n",
    "\n",
    "Using exploration bonus for actions only resulted in faster learning in the initial phase before\n",
    "the shortcut was opened. Howerver the exploration bonus wasn't stable enough to discover and\n",
    "exploit the shortcut. The bonus affects only a single action selection and isn't able to\n",
    "systematically map out areas unvisited for a long time.\n",
    " \n",
    "Dyna-Q+ directly increases action values of unvisited areas it's able to create a map of the\n",
    "long since visited territory and plan periodic repeated visits to these states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fff85f-4f8a-4dbf-8c6d-d9c5e477bae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('ShortcutMaze-v0')\n",
    "env.seed(3); np.random.seed(3)\n",
    "q2, policy2, history2 = dyna_q_plus(env, n=50, num_episodes=400, alpha=0.5, gamma=0.95,\n",
    "                                    kappa=5e-4, action_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c08dbba-0909-4ced-a8d8-fd90e333894f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(env, q2, policy2, \"Dyna-Q+ with Action Exploration Bonus\")\n",
    "\n",
    "plot_learning([history0, history1, history2],\n",
    "              [\"Dyna-Q\", \"Dyna-Q+\", \"Dyna-Q+ with Action Exploration Bonus\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
